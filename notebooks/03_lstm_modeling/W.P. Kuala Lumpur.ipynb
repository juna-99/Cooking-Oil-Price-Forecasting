{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d060fd0",
   "metadata": {
    "id": "6d060fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\afrin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82cda41",
   "metadata": {
    "id": "f82cda41",
    "outputId": "5bf71891-379a-46db-9413-79c1d6bb9015"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>premise_code</th>\n",
       "      <th>price</th>\n",
       "      <th>premise_type</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>6.65</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Melaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>6.65</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Melaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>3</td>\n",
       "      <td>6.65</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Melaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>3</td>\n",
       "      <td>6.65</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Melaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>3</td>\n",
       "      <td>6.65</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Melaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183650</th>\n",
       "      <td>2023-12-04</td>\n",
       "      <td>20903</td>\n",
       "      <td>6.90</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183651</th>\n",
       "      <td>2023-12-05</td>\n",
       "      <td>20903</td>\n",
       "      <td>6.90</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183652</th>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>20903</td>\n",
       "      <td>6.90</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183653</th>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>20903</td>\n",
       "      <td>6.90</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183654</th>\n",
       "      <td>2023-12-12</td>\n",
       "      <td>20903</td>\n",
       "      <td>6.90</td>\n",
       "      <td>Pasar Raya / Supermarket</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183655 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  premise_code  price              premise_type     state\n",
       "0      2022-01-01             3   6.65  Pasar Raya / Supermarket    Melaka\n",
       "1      2022-01-03             3   6.65  Pasar Raya / Supermarket    Melaka\n",
       "2      2022-01-10             3   6.65  Pasar Raya / Supermarket    Melaka\n",
       "3      2022-02-14             3   6.65  Pasar Raya / Supermarket    Melaka\n",
       "4      2022-02-21             3   6.65  Pasar Raya / Supermarket    Melaka\n",
       "...           ...           ...    ...                       ...       ...\n",
       "183650 2023-12-04         20903   6.90  Pasar Raya / Supermarket  Selangor\n",
       "183651 2023-12-05         20903   6.90  Pasar Raya / Supermarket  Selangor\n",
       "183652 2023-12-06         20903   6.90  Pasar Raya / Supermarket  Selangor\n",
       "183653 2023-12-07         20903   6.90  Pasar Raya / Supermarket  Selangor\n",
       "183654 2023-12-12         20903   6.90  Pasar Raya / Supermarket  Selangor\n",
       "\n",
       "[183655 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('Data.csv')\n",
    "\n",
    "# Ensure the date column is in datetime format\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df37b72",
   "metadata": {
    "id": "2df37b72",
    "outputId": "56d890ab-2488-44b8-f8c8-538141cc5f4c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filter for W.P._Kuala_Lumpur state\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m W\u001b[38;5;241m.\u001b[39mP\u001b[38;5;241m.\u001b[39m_Kuala_Lumpur_data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW.P._Kuala_Lumpur\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m W\u001b[38;5;241m.\u001b[39mP\u001b[38;5;241m.\u001b[39m_Kuala_Lumpur_data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter for W.P._Kuala_Lumpur state\n",
    "W.P._Kuala_Lumpur_data = data[data['state'] == 'W.P._Kuala_Lumpur']\n",
    "W.P._Kuala_Lumpur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f9470",
   "metadata": {
    "id": "388f9470",
    "outputId": "081276b8-e651-465a-b1b8-57fd05711eaf"
   },
   "outputs": [],
   "source": [
    "# Aggregate the data: average price per day\n",
    "aggregated_data = W.P._Kuala_Lumpur_data.groupby('date')['price'].mean().reset_index()\n",
    "aggregated_data = aggregated_data.sort_values('date')\n",
    "\n",
    "# Set date as index\n",
    "aggregated_data.set_index('date', inplace=True)\n",
    "\n",
    "# Generate a full date range\n",
    "full_date_range = pd.date_range(start=aggregated_data.index.min(), end=aggregated_data.index.max(), freq='D')\n",
    "\n",
    "# Reindex the data to this full date range\n",
    "aggregated_data = aggregated_data.reindex(full_date_range)\n",
    "\n",
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(aggregated_data.isnull().sum())\n",
    "\n",
    "# Fill missing prices\n",
    "aggregated_data['price'] = aggregated_data['price'].ffill().bfill()\n",
    "\n",
    "# Reset index to have date as a column again\n",
    "aggregated_data.reset_index(inplace=True)\n",
    "aggregated_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "# Ensure there are no missing values\n",
    "assert aggregated_data.isnull().sum().sum() == 0\n",
    "\n",
    "# Check for missing values after filling\n",
    "print(\"Missing values after filling:\")\n",
    "print(aggregated_data.isnull().sum())\n",
    "\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e005f",
   "metadata": {
    "id": "f57e005f"
   },
   "outputs": [],
   "source": [
    "# Handle outliers by capping values at the 1st and 99th percentiles\n",
    "lower_bound = aggregated_data['price'].quantile(0.01)\n",
    "upper_bound = aggregated_data['price'].quantile(0.99)\n",
    "aggregated_data['price'] = np.clip(aggregated_data['price'], lower_bound, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe73e01",
   "metadata": {
    "id": "afe73e01"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "aggregated_data['price'] = scaler.fit_transform(aggregated_data['price'].values.reshape(-1, 1))\n",
    "\n",
    "# Convert the data to a supervised learning problem\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d81a2",
   "metadata": {
    "id": "ff7d81a2"
   },
   "outputs": [],
   "source": [
    "# Define the time step\n",
    "time_step = 30\n",
    "\n",
    "# Create the dataset\n",
    "X, Y = create_dataset(aggregated_data['price'].values.reshape(-1, 1), time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f595c",
   "metadata": {
    "id": "eb4f595c"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
    "Y_train, Y_test = Y[0:train_size], Y[train_size:len(Y)]\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956df86",
   "metadata": {
    "id": "5956df86",
    "outputId": "67e7cf91-4bb8-4307-e19f-5f7a728dd5f5"
   },
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee9ca6",
   "metadata": {
    "id": "66ee9ca6",
    "outputId": "e42a81d6-77b1-41b4-95b0-b82a32a65bc8"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, Y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "Y_train = scaler.inverse_transform([Y_train])\n",
    "Y_test = scaler.inverse_transform([Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0bd69",
   "metadata": {
    "id": "08f0bd69",
    "outputId": "86ce388e-c521-48a5-bd11-3a33e5c2ae58"
   },
   "outputs": [],
   "source": [
    "# Calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Calculate error metrics for the training data\n",
    "train_mae = mean_absolute_error(Y_train[0], train_predict[:, 0])\n",
    "train_rmse = np.sqrt(mean_squared_error(Y_train[0], train_predict[:, 0]))\n",
    "train_mape = mean_absolute_percentage_error(Y_train[0], train_predict[:, 0])\n",
    "\n",
    "# Calculate error metrics for the testing data\n",
    "test_mae = mean_absolute_error(Y_test[0], test_predict[:, 0])\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test[0], test_predict[:, 0]))\n",
    "test_mape = mean_absolute_percentage_error(Y_test[0], test_predict[:, 0])\n",
    "\n",
    "# Print the error metrics\n",
    "print(f'Training MAE: {train_mae}')\n",
    "print(f'Training RMSE: {train_rmse}')\n",
    "print(f'Training MAPE: {train_mape}%')\n",
    "\n",
    "print(f'Testing MAE: {test_mae}')\n",
    "print(f'Testing RMSE: {test_rmse}')\n",
    "print(f'Testing MAPE: {test_mape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1065c9",
   "metadata": {
    "id": "5b1065c9"
   },
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "train_predict_plot = np.empty_like(aggregated_data['price'])\n",
    "train_predict_plot[:] = np.nan\n",
    "train_predict_plot[time_step:len(train_predict) + time_step] = train_predict.flatten()\n",
    "\n",
    "test_predict_plot = np.empty_like(aggregated_data['price'])\n",
    "test_predict_plot[:] = np.nan\n",
    "\n",
    "# Ensure the indices and lengths match correctly\n",
    "test_start_idx = len(train_predict) + time_step\n",
    "test_end_idx = test_start_idx + len(test_predict)\n",
    "\n",
    "# Adjust the end index to match the length of test_predict\n",
    "if test_end_idx > len(aggregated_data):\n",
    "    test_end_idx = len(aggregated_data)\n",
    "\n",
    "test_predict_plot[test_start_idx:test_end_idx] = test_predict.flatten()[:(test_end_idx - test_start_idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e38e2a",
   "metadata": {
    "id": "33e38e2a",
    "outputId": "24372190-6e1a-40ca-888d-6462fb5a4d09"
   },
   "outputs": [],
   "source": [
    "# Forecasting future values\n",
    "n_future_days = 30\n",
    "future_input = aggregated_data['price'].values[-time_step:]\n",
    "future_input = future_input.reshape((1, time_step, 1))\n",
    "\n",
    "future_predict = []\n",
    "for _ in range(n_future_days):\n",
    "    future_price = model.predict(future_input)\n",
    "    future_predict.append(future_price[0, 0])\n",
    "    future_input = np.append(future_input[:, 1:, :], future_price.reshape((1, 1, 1)), axis=1)\n",
    "\n",
    "future_predict = scaler.inverse_transform(np.array(future_predict).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0c7c1",
   "metadata": {
    "id": "fea0c7c1",
    "outputId": "5157fbee-a285-4cdb-a533-1e641215d748"
   },
   "outputs": [],
   "source": [
    "# Create future dates for plotting\n",
    "future_dates = pd.date_range(start=aggregated_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=n_future_days)\n",
    "\n",
    "# Combine the original data and the forecast\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'], scaler.inverse_transform(aggregated_data['price'].values.reshape(-1, 1)), label='Original Price')\n",
    "plt.plot(aggregated_data['date'], train_predict_plot, label='Train Predict')\n",
    "plt.plot(aggregated_data['date'], test_predict_plot, label='Test Predict')\n",
    "plt.plot(future_dates, future_predict, label='Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('W.P._Kuala_Lumpur Price Prediction and Forecast using LSTM')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a2662",
   "metadata": {
    "id": "a07a2662",
    "outputId": "55a252c9-96cd-4885-e7e2-c0270f24a4e8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute Residuals\n",
    "train_residuals = Y_train[0] - train_predict[:, 0]\n",
    "test_residuals = Y_test[0] - test_predict[:, 0]\n",
    "\n",
    "# Plot Residuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'][:len(train_residuals)], train_residuals, label='Train Residuals')\n",
    "plt.plot(aggregated_data['date'][len(train_residuals):len(train_residuals) + len(test_residuals)], test_residuals, label='Test Residuals')\n",
    "plt.title('Residuals')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Autocorrelation of Residuals\n",
    "plot_acf(train_residuals)\n",
    "plt.title('Autocorrelation of Train Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(train_residuals)\n",
    "plt.title('Partial Autocorrelation of Train Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_acf(test_residuals)\n",
    "plt.title('Autocorrelation of Test Residuals')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(test_residuals)\n",
    "plt.title('Partial Autocorrelation of Test Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcd418",
   "metadata": {
    "id": "6bbcd418"
   },
   "source": [
    "### TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdb19a",
   "metadata": {
    "id": "8cbdb19a"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b09ff",
   "metadata": {
    "id": "5b4b09ff",
    "outputId": "1d3603b3-8a90-4916-f6f3-dc99ee465b42"
   },
   "outputs": [],
   "source": [
    "# Create and train LSTM model function\n",
    "def create_and_train_model(X_train, Y_train, X_val, Y_val, epochs=50, batch_size=32):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Use EarlyStopping to stop training when validation loss stops improving\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Data preparation (assuming aggregated_data is already loaded and preprocessed)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(aggregated_data['price'].values.reshape(-1, 1))\n",
    "\n",
    "time_step = 60\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(data, time_step):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        X.append(data[i:(i+time_step), 0])\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "test_size = len(scaled_data) - train_size\n",
    "train_data, test_data = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]\n",
    "\n",
    "X_train, Y_train = create_dataset(train_data, time_step)\n",
    "X_test, Y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "val_size = int(len(X_train) * 0.2)\n",
    "X_train_final, X_val = X_train[:-val_size], X_train[-val_size:]\n",
    "Y_train_final, Y_val = Y_train[:-val_size], Y_train[-val_size:]\n",
    "\n",
    "# Ensure the sizes are correct\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"Y_train_final shape: {Y_train_final.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"Y_val shape: {Y_val.shape}\")\n",
    "\n",
    "# Train the model with more epochs and early stopping\n",
    "model, history = create_and_train_model(X_train_final, Y_train_final, X_val, Y_val, epochs=100, batch_size=32)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "Y_train = scaler.inverse_transform([Y_train])\n",
    "Y_test = scaler.inverse_transform([Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525f453",
   "metadata": {
    "id": "2525f453",
    "outputId": "a0f6ce2a-7871-4a26-f094-5752cd4ccd21"
   },
   "outputs": [],
   "source": [
    "# Calculate error metrics for the training data\n",
    "train_mae = mean_absolute_error(Y_train[0], train_predict[:, 0])\n",
    "train_rmse = np.sqrt(mean_squared_error(Y_train[0], train_predict[:, 0]))\n",
    "train_mape = mean_absolute_percentage_error(Y_train[0], train_predict[:, 0])\n",
    "\n",
    "# Calculate error metrics for the testing data\n",
    "test_mae = mean_absolute_error(Y_test[0], test_predict[:, 0])\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test[0], test_predict[:, 0]))\n",
    "test_mape = mean_absolute_percentage_error(Y_test[0], test_predict[:, 0])\n",
    "\n",
    "# Print the error metrics\n",
    "print(f'Training MAE: {train_mae}')\n",
    "print(f'Training RMSE: {train_rmse}')\n",
    "print(f'Training MAPE: {train_mape}%')\n",
    "\n",
    "print(f'Testing MAE: {test_mae}')\n",
    "print(f'Testing RMSE: {test_rmse}')\n",
    "print(f'Testing MAPE: {test_mape}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8df2d",
   "metadata": {
    "id": "2dd8df2d",
    "outputId": "2769d4b3-8d2d-4438-ca2e-002c13062c61",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ensure train_predict and test_predict are flattened\n",
    "train_predict_flat = train_predict.flatten()\n",
    "test_predict_flat = test_predict.flatten()\n",
    "\n",
    "# Initialize prediction arrays\n",
    "train_predict_plot = np.empty_like(scaled_data[:, 0])\n",
    "train_predict_plot[:] = np.nan\n",
    "test_predict_plot = np.empty_like(scaled_data[:, 0])\n",
    "test_predict_plot[:] = np.nan\n",
    "\n",
    "# Assign the predictions to the correct locations in the arrays\n",
    "train_predict_plot[time_step:len(train_predict_flat) + time_step] = train_predict_flat\n",
    "test_start_idx = len(train_predict_flat) + time_step\n",
    "test_end_idx = test_start_idx + len(test_predict_flat)\n",
    "test_predict_plot[test_start_idx:test_end_idx] = test_predict_flat\n",
    "\n",
    "# Inverse transform the predictions and original price to the original scale\n",
    "train_predict_plot = scaler.inverse_transform(train_predict_plot.reshape(-1, 1)).flatten()\n",
    "test_predict_plot = scaler.inverse_transform(test_predict_plot.reshape(-1, 1)).flatten()\n",
    "original_price = scaler.inverse_transform(scaled_data).flatten()\n",
    "\n",
    "# Forecasting future values\n",
    "n_future_days = 30\n",
    "future_input = scaled_data[-time_step:]\n",
    "future_input = future_input.reshape((1, time_step, 1))\n",
    "\n",
    "future_predict = []\n",
    "for _ in range(n_future_days):\n",
    "    future_price = model.predict(future_input)\n",
    "    future_predict.append(future_price[0, 0])\n",
    "    future_input = np.append(future_input[:, 1:, :], future_price.reshape((1, 1, 1)), axis=1)\n",
    "\n",
    "future_predict = scaler.inverse_transform(np.array(future_predict).reshape(-1, 1))\n",
    "\n",
    "# Align forecast index with test data index\n",
    "forecast_dates = pd.date_range(start=aggregated_data['date'].iloc[-1] + pd.Timedelta(days=1), periods=n_future_days, freq='D')\n",
    "\n",
    "# Plotting the forecast\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'], original_price, label='Original Price')\n",
    "plt.plot(aggregated_data['date'], train_predict_plot, label='Train Predict')\n",
    "plt.plot(aggregated_data['date'], test_predict_plot, label='Test Predict')\n",
    "plt.plot(forecast_dates, future_predict, label='Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('W.P._Kuala_Lumpur Price Prediction and Forecast using LSTM')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbc723",
   "metadata": {
    "id": "9bdbc723"
   },
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0377d7c",
   "metadata": {
    "id": "f0377d7c"
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "data = pd.read_csv('Data.csv')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "W.P._Kuala_Lumpur_data = data[data['state'] == 'W.P._Kuala_Lumpur']\n",
    "aggregated_data = W.P._Kuala_Lumpur_data.groupby('date')['price'].mean().reset_index()\n",
    "aggregated_data = aggregated_data.sort_values('date')\n",
    "\n",
    "# Set date as index\n",
    "aggregated_data.set_index('date', inplace=True)\n",
    "\n",
    "# Generate a full date range\n",
    "full_date_range = pd.date_range(start=aggregated_data.index.min(), end=aggregated_data.index.max(), freq='D')\n",
    "aggregated_data = aggregated_data.reindex(full_date_range)\n",
    "\n",
    "# Fill missing prices\n",
    "aggregated_data['price'] = aggregated_data['price'].ffill()\n",
    "\n",
    "# Reset index to have date as a column again\n",
    "aggregated_data.reset_index(inplace=True)\n",
    "aggregated_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "# Ensure there are no missing values\n",
    "assert aggregated_data.isnull().sum().sum() == 0\n",
    "\n",
    "# Define the dependent variable\n",
    "W.P._Kuala_Lumpur = aggregated_data['price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d4c81",
   "metadata": {
    "id": "f29d4c81"
   },
   "outputs": [],
   "source": [
    "# Function to create and train the LSTM model\n",
    "def create_and_train_model(X_train, Y_train, X_val, Y_val, epochs=100, batch_size=32):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Use EarlyStopping to stop training when validation loss stops improving\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc86365",
   "metadata": {
    "id": "edc86365",
    "outputId": "c1a48b8f-5693-4f67-dfd4-d901ed98832d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to perform cross-validation\n",
    "def rolling_forecast_cv(data, n_splits, time_step, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Perform rolling forecast cross-validation on time series data using LSTM.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.Series, the time series data\n",
    "    - n_splits: int, the number of splits\n",
    "    - time_step: int, the time step for the LSTM model\n",
    "\n",
    "    Returns:\n",
    "    - A tuple of lists containing MAE, RMSE, and MAPE for each split\n",
    "    \"\"\"\n",
    "    mae_list = []\n",
    "    rmse_list = []\n",
    "    mape_list = []\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "        # Scale the data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "        test_scaled = scaler.transform(test.values.reshape(-1, 1))\n",
    "\n",
    "        # Prepare the data for LSTM\n",
    "        X_train, Y_train = [], []\n",
    "        for i in range(time_step, len(train_scaled)):\n",
    "            X_train.append(train_scaled[i-time_step:i, 0])\n",
    "            Y_train.append(train_scaled[i, 0])\n",
    "        X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "        X_test, Y_test = [], []\n",
    "        for i in range(time_step, len(test_scaled)):\n",
    "            X_test.append(test_scaled[i-time_step:i, 0])\n",
    "            Y_test.append(test_scaled[i, 0])\n",
    "        X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        # Train the model\n",
    "        model, history = create_and_train_model(X_train, Y_train, X_test, Y_test, epochs, batch_size)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Inverse transform the predictions and true values\n",
    "        train_predict_inv = scaler.inverse_transform(train_predict)\n",
    "        Y_train_inv = scaler.inverse_transform(Y_train.reshape(-1, 1))\n",
    "\n",
    "        test_predict_inv = scaler.inverse_transform(test_predict)\n",
    "        Y_test_inv = scaler.inverse_transform(Y_test.reshape(-1, 1))\n",
    "\n",
    "        # Calculate error metrics\n",
    "        train_mae = mean_absolute_error(Y_train_inv, train_predict_inv)\n",
    "        train_rmse = np.sqrt(mean_squared_error(Y_train_inv, train_predict_inv))\n",
    "        train_mape = np.mean(np.abs((Y_train_inv - train_predict_inv) / Y_train_inv)) * 100\n",
    "\n",
    "        test_mae = mean_absolute_error(Y_test_inv, test_predict_inv)\n",
    "        test_rmse = np.sqrt(mean_squared_error(Y_test_inv, test_predict_inv))\n",
    "        test_mape = np.mean(np.abs((Y_test_inv - test_predict_inv) / Y_test_inv)) * 100\n",
    "\n",
    "        # Append the results\n",
    "        mae_list.append(test_mae)\n",
    "        rmse_list.append(test_rmse)\n",
    "        mape_list.append(test_mape)\n",
    "\n",
    "    return mae_list, rmse_list, mape_list\n",
    "\n",
    "# Perform cross-validation\n",
    "n_splits = 5\n",
    "time_step = 60\n",
    "mae_list, rmse_list, mape_list = rolling_forecast_cv(W.P._Kuala_Lumpur, n_splits, time_step, epochs=100, batch_size=32)\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"MAE:\", mae_list)\n",
    "print(\"RMSE:\", rmse_list)\n",
    "print(\"MAPE:\", mape_list)\n",
    "print(\"Average MAE:\", np.mean(mae_list))\n",
    "print(\"Average RMSE:\", np.mean(rmse_list))\n",
    "print(\"Average MAPE:\", np.mean(mape_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d5399",
   "metadata": {
    "id": "8e6d5399"
   },
   "source": [
    "### MODEL VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8b79e",
   "metadata": {
    "id": "4ce8b79e",
    "outputId": "a2a5332b-bb52-4483-e7e0-42f53de17663"
   },
   "outputs": [],
   "source": [
    "# Plot MAE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), mae_list, marker='o', label='MAE')\n",
    "plt.xlabel('Cross-Validation Split')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Cross-Validation MAE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), rmse_list, marker='o', label='RMSE')\n",
    "plt.xlabel('Cross-Validation Split')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.title('Cross-Validation RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot MAPE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), mape_list, marker='o', label='MAPE')\n",
    "plt.xlabel('Cross-Validation Split')\n",
    "plt.ylabel('Mean Absolute Percentage Error (%)')\n",
    "plt.title('Cross-Validation MAPE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c3e52",
   "metadata": {
    "id": "592c3e52",
    "outputId": "a50aec68-3916-4075-e7b7-0ec4a74cd56f"
   },
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "train_predict_plot = np.empty(len(scaled_data))\n",
    "train_predict_plot[:] = np.nan\n",
    "train_predict_plot[time_step:len(train_predict) + time_step] = train_predict.flatten()\n",
    "\n",
    "test_predict_plot = np.empty(len(scaled_data))\n",
    "test_predict_plot[:] = np.nan\n",
    "test_start_idx = len(train_predict) + (time_step * 2)\n",
    "test_end_idx = test_start_idx + len(test_predict)\n",
    "test_predict_plot[test_start_idx:test_end_idx] = test_predict.flatten()\n",
    "\n",
    "# Plot the forecast\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(scaler.inverse_transform(scaled_data.reshape(-1, 1)), label='Original Price')\n",
    "plt.plot(train_predict_plot, label='Train Predict')\n",
    "plt.plot(test_predict_plot, label='Test Predict')\n",
    "plt.plot(np.arange(len(scaled_data), len(scaled_data)+n_future_days), future_predict, label='Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('W.P._Kuala_Lumpur Price Prediction and Forecast using LSTM')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8ecf4",
   "metadata": {
    "id": "30e8ecf4",
    "outputId": "3790143d-0cf8-4277-9799-84abbbbeffc1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_residuals, label='Train Residuals')\n",
    "plt.plot(np.arange(len(train_residuals), len(train_residuals) + len(test_residuals)), test_residuals, label='Test Residuals')\n",
    "plt.title('Residuals')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation of Train Residuals\n",
    "plot_acf(train_residuals)\n",
    "plt.title('Autocorrelation of Train Residuals')\n",
    "plt.show()\n",
    "plot_pacf(train_residuals)\n",
    "plt.title('Partial Autocorrelation of Train Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation of Test Residuals\n",
    "plot_acf(test_residuals)\n",
    "plt.title('Autocorrelation of Test Residuals')\n",
    "plt.show()\n",
    "plot_pacf(test_residuals)\n",
    "plt.title('Partial Autocorrelation of Test Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c7045",
   "metadata": {
    "id": "6d3c7045"
   },
   "outputs": [],
   "source": [
    "# Combine the original price data and the predictions into a single DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'date': aggregated_data['date'],\n",
    "    'original': scaler.inverse_transform(aggregated_data['price'].values.reshape(-1, 1)).flatten(),\n",
    "    'train': train_predict_plot,\n",
    "    'test': test_predict_plot\n",
    "})\n",
    "\n",
    "# Append the future predictions to the DataFrame\n",
    "future_df = pd.DataFrame({\n",
    "    'date': future_dates,\n",
    "    'original': np.nan,\n",
    "    'train': np.nan,\n",
    "    'test': np.nan,\n",
    "    'forecast': future_predict.flatten()\n",
    "})\n",
    "\n",
    "# Combine both dataframes\n",
    "results = pd.concat([results, future_df], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv('W.P._Kuala_Lumpur.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e140b5",
   "metadata": {
    "id": "57e140b5"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save metrics to a CSV file\n",
    "metrics = {\n",
    "    'Training MAE': train_mae,\n",
    "    'Training RMSE': train_rmse,\n",
    "    'Training MAPE': train_mape,\n",
    "    'Testing MAE': test_mae,\n",
    "    'Testing RMSE': test_rmse,\n",
    "    'Testing MAPE': test_mape\n",
    "}\n",
    "\n",
    "with open('W.P._Kuala_Lumpur_metrics.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for key, value in metrics.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "# Save cross-validation results to a CSV file\n",
    "cv_results = {\n",
    "    'MAE': mae_list,\n",
    "    'RMSE': rmse_list,\n",
    "    'MAPE': mape_list\n",
    "}\n",
    "\n",
    "with open('W.P._Kuala_Lumpur_cv_results.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Split', 'MAE', 'RMSE', 'MAPE'])\n",
    "    for i in range(n_splits):\n",
    "        writer.writerow([i+1, mae_list[i], rmse_list[i], mape_list[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db341f",
   "metadata": {
    "id": "24db341f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
